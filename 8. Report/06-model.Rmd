---
output:
  pdf_document: default
  html_document: default
bibliography:
- book.bib
- packages.bib
---
# Fitting and analysing a model {#modelling}

```{r six-setup,message=FALSE,warning=FALSE,echo=FALSE}
library(dplyr)
library(readr)
library(stringr)
library(here)
library(auspol)
library(flextable)
library(ggplot2)
library(fs)
library(tidymodels)
library(flextable)
library(vip)

source(here::here("formatting_defaults.R"))
project_path <- fs::path(here(),"..")

vars_to_keep <- c(ls(),"vars_to_keep")


```

As mentioned in the previous section, this requires fitting both a classification and regularised regression model.\\


## Cluster classification {#clusterclassifier}

Although in section XXX, clusters were obtained using HDBSCAN, which can be used to map new data points into the existing clusters, a different approach has been taken -  to "reverse engineer" the clusters by training a classification model. The intent behind this is to leverage the trained model to identify the main contributors to the classification.

Different models where tried, starting with a basic tree partitioning. After trial and error, **random forest** was the chosen algorithm. The model training with:

* Census data from 2007 to 2016 (mirroring elections between 2006 to 2016) was used from training and testing. 
* Values for demographic attributes where centred around the overall percentage for said attribute, for the respective cluster.
* clusters previously obtained with HDBSCAN were used as the response.
* Since year has been "discounted", all values will considered one pool to divide, i.e. year will be ignored. An assumption has been made that the period in question is short enough to drastically affect the clustering model. If demographic values change - cluster assignment (for instance because of re-distribution), the effect is similar to being a different electorate.


The initial fitting produces the results presented in table XXX

```{r}
clustering_folder <-path(project_path,"6. Modelling","clustering_models")
ranger1 <- readRDS(path(clustering_folder,"clustering_ranger.rds"))

ranger1 |>
    collect_metrics() |>
  select(.metric,.estimate) |>
  flex_default("First Model","ranger11")
```

```{r}
ranger1$.predictions[[1]] |>
  select(all_of(c(".pred_class","cluster"))) |>
  mutate(correct=(.pred_class==cluster)) |>
  count(cluster,correct) |>
  group_by(cluster)      |>
  mutate(n=n/sum(n))     |>
  pivot_wider(names_from="correct",values_from=n) |>
   flex_default("First Model","ranger12")
```

```{r}

extract_workflow(ranger1) |>
  extract_fit_parsnip() |>
  vip()
```

From the chart above, it is possible to see that only a handful of variables have a significant contribution to the cluster selection. Aiming for simplification , a random forest model with reduced variables was also trained , achieving similar results in accuracy and variable importance. 




```{r}
ranger2 <- readRDS(path(clustering_folder,"clustering_ranger_redux.rds"))

ranger2 |>
    collect_metrics() |>
  select(.metric,.estimate) |>
  flex_default("First Model","ranger11")
```


```{r}
ranger1$.predictions[[1]] |>
  select(all_of(c(".pred_class","cluster"))) |>
  mutate(correct=(.pred_class==cluster)) |>
  count(cluster,correct) |>
  group_by(cluster)      |>
  mutate(n=n/sum(n))     |>
  pivot_wider(names_from="correct",values_from=n) |>
   flex_default("First Model","ranger12")
```


```{r}

extract_workflow(ranger2) |>
  extract_fit_parsnip() |>
  vip()
```

Looking at variable importance , it is possible to appreciate that cluster placement can be driven:

Location in a large metropolitan area or the regions.
Population density, (type of household)
Life stage (relationship)
Wealth (type of household ownership)
Multicultural make up of the area - first and second generation migrants are more likely to be bilingual - thus the proportion of monolingual people is a  proxy variable for this.

This picture fits with the media narrative about differences in the electorate (quote).

```{r}
 rm(list=ls()[!(ls() %in% vars_to_keep)])
```


## Regularised regression {#glmnet-regression}

Due to the large number of variables, the first step is to see if it is possible to identify which factor may influence. For this, a Lasso regression was conducted with the sole intent of variable selection. The relevant variables are presented in figure XYZ.

Note what are the main influences for each cluster - consolidate for each cluster in one picture

```{r}

enet_folder <- path(project_path,"6. Modelling","elastic_net_eval")

regression0 <- readRDS(path(enet_folder,"cluster_delta_cluster_cluster0.rds"))
regression1 <- readRDS(path(enet_folder,"cluster_delta_cluster_cluster1.rds"))
regression2 <- readRDS(path(enet_folder,"cluster_delta_cluster_cluster2.rds"))


```

