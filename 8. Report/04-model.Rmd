---
output:
  pdf_document: default
  html_document: default
bibliography:
- book.bib
- packages.bib
---

# Fitting and analysing a model {#modelling}

As mentioned in the previous section, this exercise requires fitting both a classification and regularised regression model.

## Cluster classification {#clusterclassifier}

Although HDBSCAN can be used to map new data points into the existing clusters, a different approach has been taken: to "reverse engineer" the clusters by training a classification model. The intent behind this is to leverage the trained model to identify the main contributors to the classification.

Different models were tried, starting with basic tree partitioning. After a couple of trials, a **random forest** model was selected. The algorithm was trained with:

-   Census data from 2007 to 2016 (mirroring elections between 2006 to 2016), which was used for training and validation.
-   Values for demographic attributes, which were centred around the overall percentage for said attribute, for the respective cluster.
-   Clusters previously obtained with HDBSCAN, used as the response variable.
-   Since the year has been "discounted", all values will be considered as one pool. An assumption has been made that the period in question is short enough to drastically affect the clustering model. If demographic values change - cluster assignment (for instance because of re-distribution), the effect is similar to being a different electorate.

The initial fitting produces the results presented in tables \@ref(tab:ranger11) and \@ref(tab:ranger12). A variable importance plot is also presented in figure \@ref(fig:vipranger1)

```{r ranger11}
clustering_folder <-path(project_path,"6. Modelling","clustering_models")
ranger1 <- readRDS(path(clustering_folder,"clustering_ranger.rds"))

ranger1 |>
    collect_metrics() |>
  select("Metric"=.metric,"Estimate"=.estimate) |>
  mutate(Metric = case_when(
    Metric=="accuracy" ~ "Accuracy",
    Metric=="roc_auc"  ~ "ROC AUC"
  ),
  Estimate = formatC(Estimate,digits=4,format="f")) |>
     kbl(booktabs = T,caption="First Model - Metrics",align="lr") |>
   kable_styling(latex_options = c("HOLD_position", "striped"),
                 bootstrap_options="responsive",
                 full_width=FALSE)

```

```{r ranger12}
ranger1$.predictions[[1]] |>
  select(all_of(c(".pred_class","cluster"))) |>
  mutate(correct=(.pred_class==cluster)) |>
  count(cluster,correct,name="Accuracy") |>
  group_by(cluster)      |>
  mutate(Accuracy=formatC(Accuracy/sum(Accuracy),digits=4,format="f"))   |>
  filter(correct)        |>
  select(-correct) |>
  rename("Cluster"="cluster") |>
  kbl(booktabs = T,caption="Accuracy by Cluster - First Model",align="cr") |>
  kable_styling(latex_options = c("HOLD_position", "striped"),
                 bootstrap_options="responsive",
                 full_width=FALSE)


```

```{r vipranger1, fig.cap="Variable importace - First classification model"}
extract_workflow(ranger1) |>
  extract_fit_parsnip() |>
  vip(aesthetics=list(fill="#00843D",colour="#00843D")) +
  labs(title="Variable Importance") 
```

From the chart above, it is possible to see that only a handful of variables significantly contribute to the cluster selection. Aiming for simplification, a random forest model with reduced variables was also trained, achieving similar results in accuracy and variable importance (shown in tables \@ref(tab:ranger21) and \@ref(tab:ranger22), and figure \@ref(fig:vipranger2).

```{r ranger21}
ranger2 <- readRDS(path(clustering_folder,"clustering_ranger_redux.rds"))

ranger2 |>
    collect_metrics() |>
    select("Metric"=.metric,"Estimate"=.estimate) |>
  mutate(Metric = case_when(
    Metric=="accuracy" ~ "Accuracy",
    Metric=="roc_auc"  ~ "ROC AUC"
  ),
  Estimate = formatC(Estimate,digits=4,format="f")) |>
     kbl(booktabs = T,caption="Improved Classification Model - Metrics",align="lr") |>
   kable_styling(latex_options = c("HOLD_position", "striped"),
                 bootstrap_options="responsive",
                 full_width=FALSE)

  #flex_default("First Model","ranger11")
```

```{r ranger22}
ranger2$.predictions[[1]] |>
  select(all_of(c(".pred_class","cluster"))) |>
  mutate(correct=(.pred_class==cluster)) |>
  count(cluster,correct,name="Accuracy") |>
  group_by(cluster)      |>
  mutate(Accuracy=formatC(Accuracy/sum(Accuracy),digits=4,format="f"))   |>
  filter(correct)        |>
  select(-correct) |>
  rename("Cluster"="cluster") |>
  kbl(booktabs = T,caption="Accuracy by Cluster - Improved Model",align="cr") |>
  kable_styling(latex_options = c("HOLD_position", "striped"),
                 bootstrap_options="responsive",
                 full_width=FALSE)

```

```{r vipranger2,fig.cap="Variable importace - Improved classification model"}

extract_workflow(ranger2) |>
  extract_fit_parsnip() |>
  vip(aesthetics=list(fill="#00843D",colour="#00843D")) +
  labs(title="Variable Importance") 

```

Looking at variable importance, it is possible to appreciate that cluster placement can be driven by :

- Location in a large metropolitan area or the regions.
- Population density, (type of household)
- Life stage (relationship) -Wealth (type of household ownership)
- Multicultural make-up of the area - first and second-generation migrants are more likely to be bilingual - thus the proportion of monolingual people is a proxy variable for this.

This picture fits with the media narrative about differences in the electorate (quote).

```{r}
 rm(list=ls()[!(ls() %in% keep_vars)])
```

## Regularised regression {#glmnet-regression}

Due to the large number of variables, the first step is to see if it is possible to identify which factors may be of influence. For this, a Lasso regression was conducted with the sole intent of variable selection. Then an elastic net was fitted, with the goal to optimise the root square mean error (RMSE). This process was done separately for each cluster. Although precision is not a key objective of this exercise, table \@ref(tab:bestresults) presents the best RMSE result per cluster, alongside the selected tuning parameters.

```{r}

enet_folder <- path(project_path,"6. Modelling","elastic_net_eval")
regression <- list()
for(i in 0:2){
    message(glue::glue("cluster_delta_cluster_cluster{i}.rds"))
    regression[[length(regression)+1]] <- readRDS(path(enet_folder,glue::glue("cluster_delta_cluster_cluster{i}.rds")))
}
names(regression) <- str_c("cluster ",0:2)

```


```{r bestresults}
best_parameters <- tibble()

for(i in 1:3){
  best_parameters <- bind_rows(best_parameters,
                      regression[[i]] |>  
                      filter(RMSE_Overall==min(RMSE_Overall)) |>
                      select(-coefs,-number) |>
                      add_column(cluster=i-1,.before=1) |>
                      select(cluster,alpha,lambda,
                             RMSE_Overall,RMSE_GRN,
                             RMSE_ALP,RMSE_COAL,RMSE_Other
                             ) |>
                      rename("Cluster"="cluster",
                             "⍺"      = "alpha",
                             "λ"      = "lambda") |>
                      rename_with( ~ str_remove_all(.x,"RMSE_"),.cols = contains("RMSE"))
  )     
}



best_parameters |>
  mutate(Cluster=as.character(Cluster)) |>
  mutate(across(where(is.numeric), ~ formatC(.x,digits=4,format="f"))) |>
  kbl(booktabs = T,caption="Best regression results by cluster",
      align=glue("c{str_c(rep('r',7),collapse='')}")) |>
  kable_styling(latex_options = c("HOLD_position", "striped"),
                 bootstrap_options="responsive",
                 full_width=FALSE) |>
  add_header_above(c(" "=3, "RMSE" = 5),align="l")

```

However, the main objective is to understand the coefficients for each covariate, which are presented in figures \@ref(fig:betacoefficients0),\@ref(fig:betacoefficients1) and \@ref(fig:betacoefficients2).

```{r}
#load data

dataset <- read_csv(path(project_path,"4. Data","consolidated_cluster.csv"))      |>
           filter(election_year!=2022)                       |>
           select(-any_of(c("Metro_Area")))              |>
            mutate(across(where(is.numeric), ~ replace_na(.x,0)))

clusters <- read_csv(path(project_path,"4. Data","clusters.csv"))         |>
            select(-any_of(c("Metro_Area")))

dataset <- dataset |>
           left_join(clusters,by=c("DivisionNm"="DivisionNm","election_year"="Year"))

rm(clusters)

vote <- dataset |> select(DivisionNm,Year,election_year,StateAb,Metro,cluster,all_of(party_cols))

cluster_avg <- read_csv(path(project_path,"4. Data","cluster_values.csv")) |>
             filter(Year!=2021) |>
             pivot_longer(-c(Year,cluster),
                          names_to = "Attribute",values_to="National") |>
             mutate(Attribute=str_replace_all(Attribute," - ","_"),
                    Attribute=str_replace_all(Attribute,"-","_"),
                    Attribute=str_squish(Attribute),
                    Attribute=str_replace_all(Attribute," ","_")) 

rest <- dataset |> 
        select(-all_of(party_cols)) |>
        select(-Metro,-StateAb,-election_year) |>
         pivot_longer(-c(DivisionNm,Year,cluster),
               names_to = "Attribute",values_to = "CED") |>
         mutate(Year=as.numeric(Year))                   |>
         left_join(cluster_avg,
            by=c("Year","Attribute","cluster")) |>
          select(-cluster) |>
          mutate(Value=CED-National,.keep="unused") |>
          pivot_wider(names_from = Attribute, values_from = Value)

dataset <- vote |>
           left_join(rest,by=c("DivisionNm","Year")) |>
           mutate(Division=str_c(DivisionNm,"-",election_year),.keep="unused") |>
          select(-any_of(c("Year","Household_Semi_detached"))) |>
          select(-any_of(c("StateAb","Metro")))

variances <- dataset |>
  select(-all_of(c(party_cols,"Division"))) |>
  group_by(cluster) |>
  summarise(across(everything(), ~var(.x)))   |>
  pivot_longer(-cluster,names_to="covariate",values_to="variance")

rm(rest,vote,dataset)

```

```{r betacoefficients,fig.cap="Resulting coefficients per cluster", fig.height=20, message=FALSE, warning=TRUE}
library(patchwork)

p <- list()
for(i in 1:3){
   data     <- (regression[[i]]                        |> 
                          filter(RMSE_Overall==min(RMSE_Overall)) |>
                          pull(coefs))[[1]]                       |>
                          pivot_longer(-covariate,names_to="PartyAb",values_to = "coefficient") |>
                          mutate(cluster=names(regression)[i],.before=1) |>
                left_join(variances |> filter(cluster==(i-1)),
                          by=c("covariate")) |>
                mutate(covariate=glue::glue("{covariate} ({round(variance,2)})")) |>
                mutate(covariate=str_remove_all(covariate,"\\(NA\\)")) |>
                mutate(variance = if_else(is.na(variance),0,variance)) |>
                mutate(covariate=forcats::fct_reorder(covariate,variance)) |>
                select(covariate,coefficient,PartyAb)
   
    max_coeff <- max(abs(data$coefficient))
    scale_breaks <- c(0,1,2,round(max_coeff/2,0),ceiling(max_coeff))
    scale_breaks <- c(-scale_breaks,scale_breaks)
    scale_breaks <- unique(scale_breaks)
  
    p_i <-    data |> 
                ggplot(aes(y=covariate,x=coefficient,colour=PartyAb)) +
                geom_vline(xintercept = 0) +
                geom_point(size=3.5,shape=18) +
                scale_colour_manual(values=party_palette,name="Party") +
                scale_x_continuous(trans=scales::pseudo_log_trans(), breaks=scale_breaks) +
                labs(subtitle= cluster_names[i,]$name_composite,
                     x="coefficient") +
                theme(axis.title.y =element_blank())
                

    if(i==3){
      p_i <- p_i +
            theme(legend.position = "bottom",
                  legend.direction = "horizontal")
    }else{
      p_i <- p_i + 
            theme(legend.position = "none",
                  axis.title.x = element_blank())
    }
    
  p[[length(p)+1]] <- p_i

}


```


```{r betacoefficents0,fig.cap="Resulting coefficients - cluster 0"}

p[[1]] + labs(title = "Coefficients for cluster 0",
              subtitle="Covariate's variance in brackets")

```

```{r betacoefficents1,fig.cap="Resulting coefficients - cluster 0"}

p[[2]] + labs(title = "Coefficients for cluster 1",
              subtitle="Covariate's variance in brackets")

```

```{r betacoefficents2,fig.cap="Resulting coefficients - cluster 0"}
p[[3]] + labs(title = "Coefficients for cluster 2",
              subtitle="Covariate's variance in brackets")
```


It is worth noticing that some of the selected covariates may not be relevant in all electorates, by account of their small absolute various or being relatively uniform across the segment. For this reason, the covariates in figures \@ref(fig:betacoefficients0),\@ref(fig:betacoefficients1) and \@ref(fig:betacoefficients2) have been ordered by their respective variance - when assessing their overall effect / relevance this must also be taken into account.

When looking at each cluster, it is possible to summarise the different demographic effects as follows:

-   In **cluster 0** (mostly inner metropolitan areas) political divides are drawn across wealth, religiosity (i.e. values) and generational lines.

  -   In these areas, coalition vote is associated with higher percentages of followers of Anglican, Uniting and Presbyterian churches, people on higher income and Baby Boomers.
  -   Labor vote is turn driven by followers of the Catholic Church (partially a reflection of the historic association between the Australian Catholic Church and the labour movement, and Irish and Italian migration) and Millennials. There is some association between less-advantaged populations and social and community housing.
  -   Green vote is also driven by Millennials, but unlike Labor there is a positive association with higher income groups. Green votes are also related to the irreligiosity o secular population groups.
-   In **cluster 1** (regional areas, including mid-size cities and rural areas), demographic variance is smaller. However, when it happens, it follows a different pattern from the main cities.
  - In this area, the Coalition vote has also a positive association with religiosity - this is not dissimilar to cluster 1, especially when considering that Anglicanism/Presbyterianism/Unitiarianism are the largest religious groups in the area). However, a key difference with the cities is that in case higher wealth groups have a negative association with Coalition vote.
  - Labor vote in these areas is driven by a larger proportion of Australian citizens and higher-income voters.
  - Overall, it seems there are no demographic factors influencing Green votes in these areas.
  - Interestingly, age does not rank as a variable of importance.
- As expected, **cluster 2** (metropolitan suburbia), shares some traits with their inner-city counterparts, showing the same associations along religious, age and wealth lines. However, there are a larger number of predictors associated with the multicultural make-up of the electorates. Those covariates tend to have a positive effect on Labor vote and a negative influence on Coalition and Green voting. This difference is interesting, especially considering inner city areas are as multicultural as the suburbs.

```{r}
rm(list=ls()[!(ls() %in% keep_vars)])
```
