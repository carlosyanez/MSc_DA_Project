---
output:
  pdf_document: default
  html_document: default
bibliography:
- book.bib
- packages.bib
---

# Fitting and analysing a model {#modelling}

```{r six-setup,message=FALSE,warning=FALSE,echo=FALSE}
library(dplyr)
library(readr)
library(stringr)
library(here)
library(auspol)
library(flextable)
library(ggplot2)
library(fs)
library(tidymodels)
library(flextable)
library(vip)

source(here::here("formatting_defaults.R"))
project_path <- fs::path(here(),"..")

vars_to_keep <- c(ls(),"vars_to_keep")


```

As mentioned in the previous section, this requires fitting both a classification and regularised regression model.

## Cluster classification {#clusterclassifier}

Although in section XXX, clusters were obtained using HDBSCAN, which can be used to map new data points into the existing clusters, a different approach has been taken - to "reverse engineer" the clusters by training a classification model. The intent behind this is to leverage the trained model to identify the main contributors to the classification.

Different models where tried, starting with a basic tree partitioning. After trial and error, **random forest** was the chosen algorithm. The model training with:

-   Census data from 2007 to 2016 (mirroring elections between 2006 to 2016) was used from training and testing.
-   Values for demographic attributes where centred around the overall percentage for said attribute, for the respective cluster.
-   clusters previously obtained with HDBSCAN were used as the response.
-   Since year has been "discounted", all values will considered one pool to divide, i.e. year will be ignored. An assumption has been made that the period in question is short enough to drastically affect the clustering model. If demographic values change - cluster assignment (for instance because of re-distribution), the effect is similar to being a different electorate.

The initial fitting produces the results presented in table XXX

```{r}
clustering_folder <-path(project_path,"6. Modelling","clustering_models")
ranger1 <- readRDS(path(clustering_folder,"clustering_ranger.rds"))

ranger1 |>
    collect_metrics() |>
  select(.metric,.estimate) |>
  flex_default("First Model","ranger11")
```

```{r}
ranger1$.predictions[[1]] |>
  select(all_of(c(".pred_class","cluster"))) |>
  mutate(correct=(.pred_class==cluster)) |>
  count(cluster,correct) |>
  group_by(cluster)      |>
  mutate(n=n/sum(n))     |>
  pivot_wider(names_from="correct",values_from=n) |>
   flex_default("First Model","ranger12")
```

```{r}

extract_workflow(ranger1) |>
  extract_fit_parsnip() |>
  vip()
```

From the chart above, it is possible to see that only a handful of variables have a significant contribution to the cluster selection. Aiming for simplification , a random forest model with reduced variables was also trained , achieving similar results in accuracy and variable importance.

```{r}
ranger2 <- readRDS(path(clustering_folder,"clustering_ranger_redux.rds"))

ranger2 |>
    collect_metrics() |>
  select(.metric,.estimate) |>
  flex_default("First Model","ranger11")
```

```{r}
ranger1$.predictions[[1]] |>
  select(all_of(c(".pred_class","cluster"))) |>
  mutate(correct=(.pred_class==cluster)) |>
  count(cluster,correct) |>
  group_by(cluster)      |>
  mutate(n=n/sum(n))     |>
  pivot_wider(names_from="correct",values_from=n) |>
   flex_default("First Model","ranger12")
```

```{r}

extract_workflow(ranger2) |>
  extract_fit_parsnip() |>
  vip()
```

Looking at variable importance , it is possible to appreciate that cluster placement can be driven:

Location in a large metropolitan area or the regions. Population density, (type of household) Life stage (relationship) Wealth (type of household ownership) Multicultural make up of the area - first and second generation migrants are more likely to be bilingual - thus the proportion of monolingual people is a proxy variable for this.

This picture fits with the media narrative about differences in the electorate (quote).

```{r}
 rm(list=ls()[!(ls() %in% vars_to_keep)])
```

## Regularised regression {#glmnet-regression}

Due to the large number of variables, the first step is to see if it is possible to identify which factor may influence. For this, a Lasso regression was conducted with the sole intent of variable selection. Then an elastic net was fitted, with the goal to optimise the root square mean error (RMSE). This process was done separately for each cluster. Although precision is not a key objective of this exercise, table \@ref(tab:bestresults) presents the best RMSE result per cluster, alongside the selected tuning parameters.

```{r}

enet_folder <- path(project_path,"6. Modelling","elastic_net_eval")
regression <- list()
for(i in 0:2){
    message(glue::glue("cluster_delta_cluster_cluster{i}.rds"))
    regression[[length(regression)+1]] <- readRDS(path(enet_folder,glue::glue("cluster_delta_cluster_cluster{i}.rds")))
}
names(regression) <- str_c("cluster ",0:2)

```

```{r}
best_parameters <- tibble()

for(i in 1:3){
  best_parameters <- bind_rows(best_parameters,
                      regression[[i]] |>  
                      filter(RMSE_Overall==min(RMSE_Overall)) |>
                      select(-coefs,-number) |>
                      add_column(cluster=i-1,.before=1) |>
                      select(cluster,alpha,lambda,
                             RMSE_Overall,RMSE_GRN,
                             RMSE_ALP,RMSE_COAL,RMSE_Other
                             )
  )  
}

header_key <- tibble(col_keys = colnames(best_parameters)) |>
              mutate(second_row=case_when(
                col_keys == "cluster"  ~ "Cluster",
                col_keys == "alpha"    ~ "⍺",
                col_keys == "lambda"   ~ "λ",
                str_detect(col_keys,"RMSE")  ~  str_remove_all(col_keys,"RMSE_")
              )) |>
              mutate(first_row=case_when(
                col_keys == "cluster"  ~ "Cluster",
                col_keys == "alpha"    ~ "⍺",
                col_keys == "lambda"   ~ "λ",
                str_detect(col_keys,"RMSE")  ~   "RMSE"
              )) |>
            select(col_keys,first_row,second_row) 



best_parameters |>
  mutate(cluster=as.character(cluster)) |>
  mutate(across(where(is.numeric), ~ round(.x,4))) |>
  flex_default("Best Results for each cluster","bestresults") |>
  set_header_df( mapping = header_key, key = "col_keys" )     |>
  merge_v(part = "header", j = 1:3) |>
  merge_h(part = "header", i = 1) |>
  theme_booktabs(bold_header = TRUE) |>
  align(align = "center", part = "header") |>
  vline(j = c(1, 3), border = fp_border_default()) 
  

```

However, the main objective is to undestand the coefficients for each covariate, which are presented in figure \@ref(fig:betacoefficients).

```{r}
#load data


dataset <- read_csv(path(project_path,"4. Data","consolidated_cluster.csv"))      |>
           filter(election_year!=2022)                       |>
           select(-any_of(c("Metro_Area")))              |>
            mutate(across(where(is.numeric), ~ replace_na(.x,0)))


clusters <- read_csv(path(project_path,"4. Data","clusters.csv"))         |>
            select(-any_of(c("Metro_Area")))


dataset <- dataset |>
           left_join(clusters,by=c("DivisionNm"="DivisionNm","election_year"="Year"))

rm(clusters)


vote <- dataset |> select(DivisionNm,Year,election_year,StateAb,Metro,cluster,all_of(party_cols))

cluster_avg <- read_csv(path(project_path,"4. Data","cluster_values.csv")) |>
             filter(Year!=2021) |>
             pivot_longer(-c(Year,cluster),
                          names_to = "Attribute",values_to="National") |>
             mutate(Attribute=str_replace_all(Attribute," - ","_"),
                    Attribute=str_replace_all(Attribute,"-","_"),
                    Attribute=str_squish(Attribute),
                    Attribute=str_replace_all(Attribute," ","_")) 


rest <- dataset |> 
        select(-all_of(party_cols)) |>
        select(-Metro,-StateAb,-election_year) |>
         pivot_longer(-c(DivisionNm,Year,cluster),
               names_to = "Attribute",values_to = "CED") |>
         mutate(Year=as.numeric(Year))                   |>
         left_join(cluster_avg,
            by=c("Year","Attribute","cluster")) |>
          select(-cluster) |>
          mutate(Value=CED-National,.keep="unused") |>
          pivot_wider(names_from = Attribute, values_from = Value)


dataset <- vote |>
           left_join(rest,by=c("DivisionNm","Year")) |>
           mutate(Division=str_c(DivisionNm,"-",election_year),.keep="unused") |>
          select(-any_of(c("Year","Household_Semi_detached"))) |>
          select(-any_of(c("StateAb","Metro")))


variances <- dataset |>
  select(-all_of(c(party_cols,"Division"))) |>
  group_by(cluster) |>
  summarise(across(everything(), ~var(.x)))   |>
  pivot_longer(-cluster,names_to="covariate",values_to="variance")

rm(rest,vote,dataset)

```

```{r betacoefficients,fig.cap="Resulting coefficients per cluster", fig.height=20, message=FALSE, warning=TRUE}
library(patchwork)


p <- list()
for(i in 1:3){
   data     <- (regression[[i]]                        |> 
                          filter(RMSE_Overall==min(RMSE_Overall)) |>
                          pull(coefs))[[1]]                       |>
                          pivot_longer(-covariate,names_to="PartyAb",values_to = "coefficient") |>
                          mutate(cluster=names(regression)[i],.before=1) |>
                left_join(variances |> filter(cluster==(i-1)),
                          by=c("covariate")) |>
                mutate(covariate=glue::glue("{covariate} ({round(variance,2)})")) |>
                mutate(covariate=str_remove_all(covariate,"\\(NA\\)")) |>
                mutate(variance = if_else(is.na(variance),0,variance)) |>
                mutate(covariate=forcats::fct_reorder(covariate,variance)) |>
                select(covariate,coefficient,PartyAb)
   
    max_coeff <- max(abs(data$coefficient))
    scale_breaks <- c(0,1,2,round(max_coeff/2,0),ceiling(max_coeff))
    scale_breaks <- c(-scale_breaks,scale_breaks)
    scale_breaks <- unique(scale_breaks)
  
    p_i <-    data |> 
                ggplot(aes(y=covariate,x=coefficient,colour=PartyAb)) +
                geom_vline(xintercept = 0) +
                geom_point(size=3.5,shape=18) +
                scale_colour_manual(values=party_palette,name="Party") +
                scale_x_continuous(trans=scales::pseudo_log_trans(), breaks=scale_breaks) +
                labs(subtitle= cluster_names[i,]$name_composite,
                     x="coefficient") +
                theme(axis.title.y =element_blank())
                

    if(i==3){
      p_i <- p_i +
            theme(legend.position = "bottom",
                  legend.direction = "horizontal")
    }else{
      p_i <- p_i + 
            theme(legend.position = "none",
                  axis.title.x = element_blank())
    }
    
  p[[length(p)+1]] <- p_i

}

wrap_plots(p,ncol=1) +
  plot_annotation(title="Coefficients by cluster",
                  subtitle="Covariate's variance in brackets") +
  ggh4x::force_panelsizes(rows = 16, cols = 1)

```

It is worth noticing that some of the selected covariates may not relevant in all electorates, by account of their small absolute various or being relative uniform across the segment . For this reason, the covariates in figure \@ref(fig:betacoefficients) have been ordered by their respective variance - when assessing their overall effect / relevance this must also be taken into account.

When looking at each cluster, it is possible to summarise the different demographic effects as follows:

-   In **cluster 0** (mostly inner metropolitan areas) political divides are drawn across wealth, religiosity (i.e. values) and generational lines.

    -    In these areas, coalition vote is associated with higher percentages of followers of Anglican, Uniting and Presbyterian churches, people on higher income and Baby Boomers.
    -   Labor vote is turn driven by followers of the Catholic Church (partially a reflection of the historic association between the Australian Catholic Church and the labour movement, and Irish and Italian migration) and Millennials. There is some association with less-advantaged population by social and community housing.
    -   Green vote is also driven by Millennials, but unlike Labor there is positive association with higher income groups. Green votes are also related to irreligiosity o secular population groups.

-   In **cluster 1** (regional areas, including midsize cities and rural areas), demographic variance is smaller. However when it happens, it follows a different pattern from the main cities.

    -   In this area Coalition vote has also a positive association with religiosity - this is not disimilar to cluster 1, especially when considering that Anglicanism/Presbytiarism/Unitiarism are the largest religious groups in the area) . However, a key difference with the cities is that in case higher wealth groups have a negative association with Coalition vote.
    -   Labor vote in these areas is driven by a larger roportion of Australian citizens and higher income voters.
    -   Overall, it seems there are no demographic factors influencing Green votes in these areas.
    -   Interestingly, age does not rank as a variable of importance.

-   As expected, **cluster 2** (metropolitian suburbia) , shares some traits with their inner city counterparts, showing the same associations along religious, age and wealth lines. However, there are larger number of predictors associated to the multicultural makeup of the electorates. Those covariates tend to have a positive effect on Labor vote and negative influence on Coalition and Green voting. This difference is interesting specially considering inner city areas are as multicultural as the suburbs.


```{r}
rm(list=ls())
```

